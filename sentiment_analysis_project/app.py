# app.py (vFinal v2 - Tab 2 Ph√¢n t√≠ch & G·ª£i √Ω AI theo Product ID)
import streamlit as st
import pandas as pd
import plotly.graph_objects as go
import plotly.express as px
import os
import time
import requests
import json
import traceback
from collections import Counter
from datetime import datetime # Th√™m datetime

import config
try:
    from visualization import plot_confusion_matrix, plot_training_history
    VIZ_AVAILABLE = True
except ImportError:
    VIZ_AVAILABLE = False
    print("C·∫£nh b√°o: Module 'visualization' kh√¥ng t√¨m th·∫•y.")

# Import th∆∞ vi·ªán Gemini v√† ki·ªÉm tra c·∫•u h√¨nh
gemini_configured_app = False
if config.GEMINI_API_KEY:
    try:
        import google.generativeai as genai
        genai.configure(api_key=config.GEMINI_API_KEY)
        gemini_configured_app = True
        print("Gemini OK (Streamlit).")
    except ImportError:
        print("C·∫£nh b√°o: google-generativeai ch∆∞a c√†i.")
    except Exception as e:
        print(f"C·∫£nh b√°o: L·ªói c·∫•u h√¨nh Gemini (Streamlit): {e}")
else:
    print("C·∫£nh b√°o: GEMINI_API_KEY ch∆∞a ƒë·∫∑t (Streamlit).")

# --- C·∫•u h√¨nh Trang ---
st.set_page_config(page_title="X·ª≠ l√Ω Ph·∫£n h·ªìi vFinal v2", page_icon="üí°", layout="wide")

# --- ƒê·ªãa ch·ªâ API Backend ---
API_HOST = getattr(config, 'API_HOST', '127.0.0.1')
API_PORT = getattr(config, 'API_PORT', 8000)
BACKEND_API_URL_SENTIMENT = f"http://{API_HOST}:{API_PORT}/sentiment/"
BACKEND_API_URL_PROCESS = f"http://{API_HOST}:{API_PORT}/process/"

# --- Giao di·ªán Ch√≠nh ---
st.title("üí° H·ªá th·ªëng Ph√¢n t√≠ch & X·ª≠ l√Ω Ph·∫£n h·ªìi Kh√°ch h√†ng (Product Aware) vFinal v2")
st.markdown("""
**Ch·ªçn c√°ch x·ª≠ l√Ω:**
- **Ph√¢n t√≠ch Nhanh:** Ch·ªâ l·∫•y c·∫£m x√∫c (nhanh, ƒë·ªçc/l∆∞u v√†o KB). *C√≥ th·ªÉ k√®m Product ID.*
- **X·ª≠ l√Ω Chi ti·∫øt:** L·∫•y c·∫£m x√∫c, g·ª£i √Ω & ph·∫£n h·ªìi AI (ƒë·ªçc/l√†m gi√†u KB & g·ªçi Gemini). *C√≥ th·ªÉ k√®m Product ID.*
- **X·ª≠ l√Ω H√†ng lo·∫°t:** Ph√¢n t√≠ch nhanh file CSV (l√†m n√≥ng KB), nh·∫≠n **ph√¢n t√≠ch c·∫£m x√∫c** v√† **g·ª£i √Ω AI** chi ti·∫øt theo t·ª´ng Product ID.
""")

# --- C√°c Tab ch·ª©c nƒÉng ---
tab1, tab2, tab3 = st.tabs(["üìù X·ª≠ l√Ω ƒê∆°n l·∫ª", "üìÇ X·ª≠ l√Ω H√†ng lo·∫°t (Theo S·∫£n ph·∫©m + AI)", "üìà Th√¥ng tin Model"])

# --- Tab 1: X·ª≠ l√Ω ƒê∆°n l·∫ª (Gi·ªØ nguy√™n, h·ªó tr·ª£ Product ID) ---
with tab1:
    st.header("Nh·∫≠p ph·∫£n h·ªìi c·∫ßn x·ª≠ l√Ω:")
    user_input_single = st.text_area("N·ªôi dung b√¨nh lu·∫≠n:", height=120, key="single_input_tab1_prod_final", placeholder="V√≠ d·ª•: Chi·∫øc √°o n√†y m√†u r·∫•t ƒë·∫πp!")
    product_id_input = st.text_input("M√£/T√™n S·∫£n ph·∫©m (T√πy ch·ªçn):", key="product_id_single_final", placeholder="V√≠ d·ª•: AO-001")

    col_btn1, col_btn2 = st.columns(2)

    def display_results_tab1(api_response, start_time, end_time, endpoint_name):
        st.markdown("---")
        st.subheader(f"K·∫øt qu·∫£ t·ª´ {endpoint_name}:")
        if not api_response:
            st.error(f"Kh√¥ng nh·∫≠n ƒë∆∞·ª£c ph·∫£n h·ªìi h·ª£p l·ªá t·ª´ API {endpoint_name}.")
            return
        total_time = (end_time - start_time) * 1000
        api_time = api_response.get('processing_time_ms')
        source = api_response.get('source', 'N/A')
        ai_reason = api_response.get('ai_call_reason')
        product_id_rcv = api_response.get('product_id_processed')
        col_res1, col_res2 = st.columns(2)
        with col_res1:
            st.markdown("**Ph√¢n t√≠ch C·∫£m x√∫c:**")
            sentiment = api_response.get('sentiment', 'N/A')
            confidence = api_response.get('confidence')
            try:
                label_map = getattr(config, 'TARGET_LABEL_MAP', {})
                positive_label = label_map.get(2, "T√≠ch c·ª±c")
                negative_label = label_map.get(0, "Ti√™u c·ª±c")
            except: 
                label_map = {}
                positive_label = "T√≠ch c·ª±c"
                negative_label = "Ti√™u c·ª±c"

            if sentiment == positive_label:
                st.success(f"**C·∫£m x√∫c:** {sentiment}")
            elif sentiment == negative_label:
                st.error(f"**C·∫£m x√∫c:** {sentiment}")
            else:
                st.warning(f"**C·∫£m x√∫c:** {sentiment}")

            if confidence is not None:
                st.metric(label="ƒê·ªô tin c·∫≠y", value=f"{confidence:.2%}")
            if product_id_rcv:
                st.caption(f"S·∫£n ph·∫©m ƒë√£ x·ª≠ l√Ω: {product_id_rcv}")

            st.caption(f"T.gian: {total_time:.0f}ms | API T.gian: {api_time:.0f}ms" if api_time else f"T.gian: {total_time:.0f}ms")
            source_text = {
                'cache': 'Cache KB',
                'cache_enriched': 'L√†m gi√†u KB',
                'new_sentiment_only': 'M·ªõi (Ch·ªâ Sentiment)',
                'new_full_process': 'M·ªõi (Full AI)',
                'error': 'L·ªói X·ª≠ l√Ω'
            }.get(source, source)
            st.caption(f"Ngu·ªìn: {source_text}")
            if ai_reason and source != 'cache':
                st.caption(f"Tr·∫°ng th√°i AI: {ai_reason}")
        with col_res2:
            st.markdown("**G·ª£i √Ω Ph·∫£n h·ªìi T·ª± ƒë·ªông (AI/Cache):**")
            generated_response = api_response.get('generated_response')
            is_valid_response = generated_response and isinstance(generated_response, str) and "L·ªói" not in generated_response and "ch∆∞a c·∫•u h√¨nh" not in generated_response and "kh√¥ng t·∫°o ra" not in generated_response
            if is_valid_response:
                st.text_area("N·ªôi dung:", value=generated_response, height=120, key=f"gen_resp_{source}_{int(time.time())}", disabled=False)
            elif generated_response:
                st.info(generated_response)
            else:
                st.info("Kh√¥ng c√≥.")
        st.markdown("---")
        st.markdown("**G·ª£i √Ω H√†nh ƒë·ªông N·ªôi b·ªô (AI/Cache):**")
        suggestions = api_response.get('suggestions')
        is_valid_suggestions = suggestions and isinstance(suggestions, list) and not any("L·ªói" in s or "ch∆∞a c·∫•u h√¨nh" in s for s in suggestions)
        if is_valid_suggestions:
            st.markdown("\n".join(f"- {s}" for s in suggestions))
        elif suggestions and isinstance(suggestions, list):
             st.info(suggestions[0] if suggestions else "Kh√¥ng c√≥.")
        else:
            st.info("Kh√¥ng c√≥.")

    with col_btn1:
        if st.button("‚ö° Ph√¢n t√≠ch Nhanh (ƒê·ªçc/L∆∞u KB)", key="analyze_fast_kb_final_prod", help="L·∫•y c·∫£m x√∫c, ƒë·ªçc/l∆∞u KB. K√®m Product ID n·∫øu c√≥."):
            if user_input_single and user_input_single.strip():
                start_time = time.time()
                api_response = None
                error_message = None
                payload = {"comment": user_input_single}
                if product_id_input and product_id_input.strip():
                    payload["product_id"] = product_id_input.strip()
                with st.spinner('‚ö° ƒêang ph√¢n t√≠ch nhanh & ki·ªÉm tra KB...'):
                    try:
                        response = requests.post(BACKEND_API_URL_SENTIMENT, json=payload, timeout=30)
                        response.raise_for_status()
                        api_response = response.json()
                    except requests.exceptions.Timeout:
                        error_message = f"L·ªói API /sentiment/: Timeout. M√°y ch·ªß c√≥ th·ªÉ ƒëang b·∫≠n."
                    except requests.exceptions.HTTPError as http_err:
                        error_message = f"L·ªói HTTP API /sentiment/: {http_err}. N·ªôi dung: {response.text if response else 'N/A'}"
                    except requests.exceptions.RequestException as req_err:
                        error_message = f"L·ªói k·∫øt n·ªëi API /sentiment/: {req_err}"
                    except Exception as e:
                        error_message = f"L·ªói kh√¥ng x√°c ƒë·ªãnh khi g·ªçi API /sentiment/: {e}"
                        traceback.print_exc()
                end_time = time.time()
                if error_message:
                    st.error(error_message)
                display_results_tab1(api_response, start_time, end_time, "/sentiment/")
            else:
                st.warning("‚ö†Ô∏è Vui l√≤ng nh·∫≠p b√¨nh lu·∫≠n.")

    with col_btn2:
        if st.button("‚ú® X·ª≠ l√Ω Chi ti·∫øt (KB + AI)", key="analyze_detailed_kb_final_prod", help="ƒê·ªçc KB, n·∫øu thi·∫øu -> XLM-R + Gemini -> L∆∞u/C·∫≠p nh·∫≠t KB. K√®m Product ID n·∫øu c√≥."):
            if user_input_single and user_input_single.strip():
                start_time = time.time()
                api_response = None
                error_message = None
                payload = {"comment": user_input_single}
                if product_id_input and product_id_input.strip():
                    payload["product_id"] = product_id_input.strip()
                with st.spinner('‚ú® ƒêang x·ª≠ l√Ω chi ti·∫øt... (C√≥ th·ªÉ m·∫•t v√†i ch·ª•c gi√¢y)'):
                    try:
                        response = requests.post(BACKEND_API_URL_PROCESS, json=payload, timeout=180)
                        response.raise_for_status()
                        api_response = response.json()
                    except requests.exceptions.Timeout:
                        error_message = f"L·ªói API /process/: Timeout. M√°y ch·ªß ho·∫∑c Gemini c√≥ th·ªÉ ƒëang b·∫≠n."
                    except requests.exceptions.HTTPError as http_err:
                        error_message = f"L·ªói HTTP API /process/: {http_err}. N·ªôi dung: {response.text if response else 'N/A'}"
                    except requests.exceptions.RequestException as req_err:
                        error_message = f"L·ªói k·∫øt n·ªëi API /process/: {req_err}"
                    except Exception as e:
                        error_message = f"L·ªói kh√¥ng x√°c ƒë·ªãnh khi g·ªçi API /process/: {e}"
                        traceback.print_exc()
                end_time = time.time()
                if error_message:
                    st.error(error_message)
                    st.info("M·∫πo: Ki·ªÉm tra k·∫øt n·ªëi m·∫°ng, server API backend v√† c·∫•u h√¨nh GEMINI_API_KEY.")
                display_results_tab1(api_response, start_time, end_time, "/process/")
            else:
                st.warning("‚ö†Ô∏è Vui l√≤ng nh·∫≠p b√¨nh lu·∫≠n.")

# --- Tab 2: X·ª≠ l√Ω H√†ng lo·∫°t (Ph√¢n t√≠ch & G·ª£i √Ω AI theo Product ID) ---
with tab2:
    st.header("Ph√¢n t√≠ch H√†ng lo·∫°t v√† G·ª£i √Ω AI theo S·∫£n ph·∫©m")
    st.markdown("T·∫£i l√™n file CSV c√≥ c·ªôt ch·ª©a **b√¨nh lu·∫≠n** v√† c·ªôt ch·ª©a **M√£/T√™n S·∫£n ph·∫©m** ƒë·ªÉ ph√¢n t√≠ch c·∫£m x√∫c v√† nh·∫≠n g·ª£i √Ω AI chi ti·∫øt cho t·ª´ng s·∫£n ph·∫©m.")

    comment_col_name_cfg = getattr(config, 'TEXT_COLUMN', 'comment')
    product_id_col_name_input = st.text_input(
        "Nh·∫≠p t√™n c·ªôt ch·ª©a M√£/T√™n S·∫£n ph·∫©m trong file CSV c·ªßa b·∫°n:",
        placeholder="V√≠ d·ª•: product_id, Product Name, MaSP,... (ph√¢n bi·ªát ch·ªØ hoa/th∆∞·ªùng)",
        key="product_id_col_csv"
    )

    uploaded_file_batch = st.file_uploader(
        f"Ch·ªçn file CSV (c·∫ßn c·ªôt '{comment_col_name_cfg}' v√† c·ªôt S·∫£n ph·∫©m b·∫°n v·ª´a nh·∫≠p)",
        type=["csv"],
        key="csv_product_analysis_v2"
    )
    limit_rows_batch_prod = st.number_input(
        "Gi·ªõi h·∫°n s·ªë d√≤ng x·ª≠ l√Ω (Nh·∫≠p 0 ƒë·ªÉ x·ª≠ l√Ω t·∫•t c·∫£):",
        min_value=0,
        value=50, 
        step=50,
        key="limit_rows_batch_prod_v2",
        help="ƒê·ªÉ 0 n·∫øu mu·ªën x·ª≠ l√Ω to√†n b·ªô file. C·∫©n th·∫≠n v·ªõi file l·ªõn c√≥ th·ªÉ t·ªën th·ªùi gian."
    )

    if uploaded_file_batch is not None and product_id_col_name_input.strip():
        product_id_col_actual = product_id_col_name_input.strip()
        try:
            df_batch_original = None
            with st.spinner("ƒêang ƒë·ªçc CSV..."):
                try:
                    try:
                        df_batch_original = pd.read_csv(uploaded_file_batch, encoding='utf-8-sig', low_memory=False)
                    except UnicodeDecodeError:
                        try:
                            df_batch_original = pd.read_csv(uploaded_file_batch, encoding='utf-8', low_memory=False)
                        except UnicodeDecodeError:
                            df_batch_original = pd.read_csv(uploaded_file_batch, encoding='latin-1', low_memory=False) 
                except Exception as e:
                    st.error(f"L·ªói ƒë·ªçc file CSV: {e}. Vui l√≤ng ki·ªÉm tra ƒë·ªãnh d·∫°ng file.")
                    # In ra m·ªôt ph·∫ßn n·ªôi dung file ƒë·ªÉ debug
                    uploaded_file_batch.seek(0) 
                    st.text_area("N·ªôi dung ƒë·∫ßu file (ƒë·ªÉ debug):", uploaded_file_batch.read(1000).decode('utf-8', errors='ignore'), height=150)
                    st.stop()
            
            if df_batch_original is None:
                st.error("Kh√¥ng th·ªÉ ƒë·ªçc ƒë∆∞·ª£c n·ªôi dung file CSV.")
                st.stop()

            st.success(f"‚úÖ File '{uploaded_file_batch.name}' ƒë√£ ƒë∆∞·ª£c t·∫£i th√†nh c√¥ng! (T·ªïng {len(df_batch_original)} d√≤ng)")
            st.dataframe(df_batch_original.head())

            # Ki·ªÉm tra s·ª± t·ªìn t·∫°i c·ªßa c·∫£ 2 c·ªôt (ph√¢n bi·ªát ch·ªØ hoa/th∆∞·ªùng)
            if comment_col_name_cfg not in df_batch_original.columns:
                st.error(f"L·ªói: Kh√¥ng t√¨m th·∫•y c·ªôt b√¨nh lu·∫≠n '{comment_col_name_cfg}' trong file CSV. C√°c c·ªôt hi·ªán c√≥: {', '.join(df_batch_original.columns)}")
                st.stop()
            if product_id_col_actual not in df_batch_original.columns:
                st.error(f"L·ªói: Kh√¥ng t√¨m th·∫•y c·ªôt s·∫£n ph·∫©m '{product_id_col_actual}' trong file CSV. C√°c c·ªôt hi·ªán c√≥: {', '.join(df_batch_original.columns)}")
                st.stop()

            if st.button("üìä Ph√¢n t√≠ch theo S·∫£n ph·∫©m & Nh·∫≠n G·ª£i √Ω AI", key="analyze_csv_by_product_v2"):
                if limit_rows_batch_prod > 0 and limit_rows_batch_prod < len(df_batch_original):
                    process_df_batch = df_batch_original.head(limit_rows_batch_prod).copy()
                    limit_info_batch = f"{limit_rows_batch_prod} d√≤ng ƒë·∫ßu"
                else:
                    process_df_batch = df_batch_original.copy()
                    limit_info_batch = "t·∫•t c·∫£ c√°c d√≤ng"
                
                total_to_process_batch = len(process_df_batch)
                if total_to_process_batch == 0:
                    st.warning("Kh√¥ng c√≥ d√≤ng n√†o ƒë·ªÉ x·ª≠ l√Ω.")
                    st.stop()

                st.info(f"B·∫Øt ƒë·∫ßu ph√¢n t√≠ch c·∫£m x√∫c cho {limit_info_batch} (t∆∞∆°ng t√°c v·ªõi KB)...")
                results_list_batch = []
                error_count_batch = 0
                cache_hit_count = 0
                
                start_batch_run_time = time.time()
                progress_bar_batch = st.progress(0)
                progress_text_container = st.empty()

                # L·∫•y c·∫•u h√¨nh ki·ªÉm tra AI (n·∫øu d√πng cho would_call_ai)
                conf_threshold_batch = float(getattr(config, 'CONFIDENCE_THRESHOLD', 0.80))
                check_negative_batch = bool(getattr(config, 'ALWAYS_CHECK_NEGATIVE', True)) 
                label_map_batch = getattr(config, 'TARGET_LABEL_MAP', {})
                negative_label_value_batch = ""
                for k, v in label_map_batch.items():
                    if k == 0:
                        negative_label_value_batch = v
                        break
                
                # --- B∆∞·ªõc 1: Ph√¢n t√≠ch c·∫£m x√∫c h√†ng lo·∫°t b·∫±ng API /sentiment/ ---
                for index, row in process_df_batch.iterrows():
                    current_time_str = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                    comment_text = str(row[comment_col_name_cfg]) if pd.notna(row[comment_col_name_cfg]) else ""
                    product_id_val = str(row[product_id_col_actual]) if pd.notna(row[product_id_col_actual]) and str(row[product_id_col_actual]).strip() else "N/A"

                    result_row = {
                        "original_comment": comment_text,
                        "product_id": product_id_val,
                        "sentiment": None,
                        "confidence": None,
                        "source": None,
                        "kb_has_ai_details": False,
                        "status": "Ch∆∞a x·ª≠ l√Ω",
                        "would_call_ai": False,
                        "processing_timestamp": current_time_str
                    }

                    if comment_text.strip():
                        try:
                            payload = {"comment": comment_text, "product_id": product_id_val}
                            response = requests.post(BACKEND_API_URL_SENTIMENT, json=payload, timeout=60)
                            response.raise_for_status()
                            api_data = response.json()
                            
                            result_row.update({
                                'sentiment': api_data.get('sentiment'),
                                'confidence': api_data.get('confidence'),
                                'source': api_data.get('source')
                            })
                            result_row['status'] = 'Th√†nh c√¥ng'

                            if api_data.get('source') == 'cache':
                                cache_hit_count += 1
                            
                            if api_data.get('suggestions') is not None or api_data.get('generated_response') is not None:
                                result_row['kb_has_ai_details'] = True

                            # ∆Ø·ªõc t√≠nh g·ªçi AI (would_call_ai)
                            would_call = False
                            # Tr∆∞·ªùng h·ª£p 1: M·ªõi ph√¢n t√≠ch (kh√¥ng ph·∫£i cache) V√Ä (ƒë·ªô tin c·∫≠y th·∫•p HO·∫∂C l√† ti√™u c·ª±c)
                            if api_data.get('source') != 'cache':
                                if (api_data.get('confidence') is not None and api_data.get('confidence') < conf_threshold_batch) or \
                                   (check_negative_batch and api_data.get('sentiment') == negative_label_value_batch):
                                    would_call = True
                            # Tr∆∞·ªùng h·ª£p 2: T·ª´ cache NH∆ØNG ch∆∞a c√≥ chi ti·∫øt AI
                            elif api_data.get('source') == 'cache' and not result_row['kb_has_ai_details']:
                                would_call = True
                            result_row['would_call_ai'] = would_call

                        except requests.exceptions.Timeout:
                            result_row['status'] = 'L·ªói API: Timeout'
                            error_count_batch += 1
                        except requests.exceptions.HTTPError as http_err:
                            result_row['status'] = f'L·ªói HTTP API ({http_err.response.status_code if http_err.response else "N/A"})'
                            error_count_batch += 1
                        except requests.exceptions.RequestException as e:
                            result_row['status'] = f'L·ªói k·∫øt n·ªëi API: {type(e).__name__}'
                            error_count_batch += 1
                        except Exception as e:
                            result_row['status'] = f'L·ªói kh√°c: {type(e).__name__}'
                            error_count_batch += 1
                    else:
                        result_row['status'] = 'B·ªè qua (b√¨nh lu·∫≠n r·ªóng)'
                    
                    results_list_batch.append(result_row)

                    progress_percentage = (index + 1) / total_to_process_batch
                    progress_text_container.text(f"ƒêang x·ª≠ l√Ω d√≤ng {index + 1}/{total_to_process_batch}...")
                    progress_bar_batch.progress(progress_percentage)

                end_batch_run_time = time.time()
                progress_text_container.text(f"Ho√†n th√†nh ph√¢n t√≠ch {total_to_process_batch} d√≤ng!")
                st.success(f"‚úÖ Ph√¢n t√≠ch c·∫£m x√∫c {total_to_process_batch} d√≤ng ho√†n t·∫•t sau {end_batch_run_time - start_batch_run_time:.2f} gi√¢y.")

                # --- B∆∞·ªõc 2: T·ªïng h·ª£p k·∫øt qu·∫£ v√† G·ªçi Gemini cho t·ª´ng Product ID ---
                if results_list_batch:
                    results_df_batch = pd.DataFrame(results_list_batch)
                  # --- ƒêo·∫°n code m·ªõi ƒë·ªÉ t·∫°o giao di·ªán 2x2 v√† hi·ªÉn th·ªã √¥ g·ªçi AI ---

                    # T√≠nh to√°n s·ªë l∆∞·ª£t c·∫ßn g·ªçi AI
                    ai_call_count = results_df_batch['would_call_ai'].sum()

                    st.markdown("---")
                    st.subheader("üìä Th·ªëng k√™ Chung (To√†n b·ªô File)")

                    # T·∫°o h√†ng th·ª© nh·∫•t v·ªõi 2 c·ªôt
                    col_b_stat1, col_b_stat2 = st.columns(2)
                    with col_b_stat1:
                        st.metric("T·ªïng d√≤ng ƒë√£ x·ª≠ l√Ω", total_to_process_batch)
                    with col_b_stat2:
                        st.metric("S·ªë d√≤ng g·∫∑p l·ªói API", error_count_batch, delta_color="inverse")

                    # T·∫°o h√†ng th·ª© hai v·ªõi 2 c·ªôt
                    col_b_stat3, col_b_stat4 = st.columns(2)
                    with col_b_stat3:
                        st.metric("S·ªë l·∫ßn d√πng Cache KB", cache_hit_count)
                    with col_b_stat4:
                        # ƒê√¢y l√† √¥ m·ªõi s·∫Ω hi·ªÉn th·ªã s·ªë l·∫ßn g·ªçi AI
                        st.metric(
                            "S·ªë d√≤ng c·∫ßn g·ªçi AI (∆Ø·ªõc t√≠nh)",
                            value=int(ai_call_count), 
                            help="S·ªë d√≤ng n√†y ƒë∆∞·ª£c ∆∞·ªõc t√≠nh d·ª±a tr√™n c√°c ƒëi·ªÅu ki·ªán: b√¨nh lu·∫≠n m·ªõi (ch∆∞a c√≥ trong KB) v√† l√† ti√™u c·ª±c/ƒë·ªô tin c·∫≠y th·∫•p, ho·∫∑c b√¨nh lu·∫≠n ƒë√£ c√≥ trong KB nh∆∞ng thi·∫øu g·ª£i √Ω AI chi ti·∫øt."
                        )
                        
                    # --- Dashboard T·ªïng h·ª£p ---
                    st.markdown("---")
                    st.subheader("üåü Dashboard T·ªïng h·ª£p: Ph√¢n t√≠ch C·∫£m x√∫c To√†n b·ªô File")
                    valid_sentiment_df = results_df_batch[results_df_batch['status'] == 'Th√†nh c√¥ng'].copy()
                    
                    if not valid_sentiment_df.empty:
                        sentiment_counts_total = valid_sentiment_df['sentiment'].value_counts()
                        all_labels_cfg = list(getattr(config, 'TARGET_LABEL_MAP', {0:"Ti√™u c·ª±c", 1:"Trung t√≠nh", 2:"T√≠ch c·ª±c"}).values())
                        for label in all_labels_cfg:
                            if label not in sentiment_counts_total:
                                sentiment_counts_total[label] = 0
                        
                        color_map_cfg = {"Ti√™u c·ª±c": '#DC143C', "Trung t√≠nh": '#FFD700', "T√≠ch c·ª±c": '#32CD32', "Kh√¥ng x√°c ƒë·ªãnh": "#808080"} # Th√™m m√†u cho kh√¥ng x√°c ƒë·ªãnh
                        
                        col_total_chart, col_total_stats = st.columns([2, 1])
                        with col_total_chart:
                            fig_pie_total = px.pie(
                                sentiment_counts_total,
                                names=sentiment_counts_total.index,
                                values=sentiment_counts_total.values,
                                title="T·ª∑ l·ªá C·∫£m x√∫c To√†n b·ªô File",
                                color=sentiment_counts_total.index,
                                color_discrete_map=color_map_cfg,
                                height=350
                            )
                            fig_pie_total.update_traces(textposition='inside', textinfo='percent+label')
                            st.plotly_chart(fig_pie_total, use_container_width=True)
                        
                        with col_total_stats:
                            total_valid_sentiments = sentiment_counts_total.sum()
                            st.markdown("**Th·ªëng k√™ Ph·∫£n h·ªìi:**")
                            for label, count in sentiment_counts_total.items():
                                percentage = (count / total_valid_sentiments) * 100 if total_valid_sentiments > 0 else 0
                                st.markdown(f"- **{label}:** {count} ({percentage:.1f}%)")

                    else:
                        st.warning("Kh√¥ng c√≥ d·ªØ li·ªáu c·∫£m x√∫c h·ª£p l·ªá ƒë·ªÉ hi·ªÉn th·ªã Dashboard T·ªïng h·ª£p.")

                    # --- Ph√¢n t√≠ch theo t·ª´ng Product ID ---
                    st.markdown("---")
                    st.subheader("üíé Ph√¢n t√≠ch & G·ª£i √Ω AI theo t·ª´ng S·∫£n ph·∫©m")
                    
                    # L·∫•y danh s√°ch s·∫£n ph·∫©m duy nh·∫•t t·ª´ c√°c d√≤ng x·ª≠ l√Ω th√†nh c√¥ng
                    unique_products = valid_sentiment_df['product_id'].unique()
                    
                    if not unique_products.size: # Ki·ªÉm tra xem m·∫£ng c√≥ r·ªóng kh√¥ng
                        st.warning("Kh√¥ng c√≥ s·∫£n ph·∫©m n√†o ƒë∆∞·ª£c x·ª≠ l√Ω th√†nh c√¥ng ƒë·ªÉ ph√¢n t√≠ch chi ti·∫øt.")
                    else:
                        for prod_id in unique_products:
                            if prod_id == "N/A": # C√≥ th·ªÉ b·ªè qua N/A ho·∫∑c x·ª≠ l√Ω ri√™ng
                                st.markdown(f"**K·∫øt qu·∫£ cho c√°c b√¨nh lu·∫≠n kh√¥ng c√≥ Product ID (N/A)**")
                            else:
                                st.markdown(f"**K·∫øt qu·∫£ cho S·∫£n ph·∫©m: `{prod_id}`**")

                            with st.expander(f"Xem chi ti·∫øt v√† g·ª£i √Ω AI cho '{prod_id}'", expanded=(prod_id != "N/A")):
                                prod_specific_df = valid_sentiment_df[valid_sentiment_df['product_id'] == prod_id]
                                if prod_specific_df.empty:
                                    st.write("Kh√¥ng c√≥ d·ªØ li·ªáu c·∫£m x√∫c h·ª£p l·ªá cho s·∫£n ph·∫©m n√†y.")
                                    continue

                                st.markdown(f"**T·ªïng s·ªë ph·∫£n h·ªìi h·ª£p l·ªá cho s·∫£n ph·∫©m n√†y:** {len(prod_specific_df)}")
                                sentiment_counts_prod = prod_specific_df['sentiment'].value_counts()
                                # ƒê·∫£m b·∫£o t·∫•t c·∫£ c√°c nh√£n ƒë·ªÅu c√≥
                                for label in all_labels_cfg:
                                    if label not in sentiment_counts_prod:
                                        sentiment_counts_prod[label] = 0
                                
                                col_p_chart, col_p_stats_ai = st.columns([1, 1])
                                with col_p_chart:
                                    fig_bar_prod = px.bar(
                                        sentiment_counts_prod,
                                        x=sentiment_counts_prod.index,
                                        y=sentiment_counts_prod.values,
                                        labels={'x': 'C·∫£m x√∫c', 'y': 'S·ªë l∆∞·ª£ng'},
                                        color=sentiment_counts_prod.index,
                                        color_discrete_map=color_map_cfg,
                                        text_auto=True, 
                                        height=300
                                    )
                                    fig_bar_prod.update_layout(showlegend=False, title_text=f"C·∫£m x√∫c SP: {prod_id}", title_x=0.5, xaxis_title=None, yaxis_title="S·ªë l∆∞·ª£ng")
                                    st.plotly_chart(fig_bar_prod, use_container_width=True)

                                with col_p_stats_ai:
                                    total_prod_sentiments = sentiment_counts_prod.sum()
                                    st.markdown("**Ph√¢n ph·ªëi:**")
                                    if total_prod_sentiments > 0:
                                        for label, count in sentiment_counts_prod.items():
                                            percentage = (count / total_prod_sentiments) * 100
                                            st.markdown(f"- {label}: {count} ({percentage:.1f}%)")
                                        
                                        st.markdown("---")
                                        st.markdown("**G·ª£i √Ω H√†nh ƒë·ªông (AI):**")
                                        if gemini_configured_app:
                                            # T·∫°o prompt t√≥m t·∫Øt
                                            summary_parts = []
                                            for label, count in sentiment_counts_prod.items():
                                                if count > 0:
                                                    percentage = (count / total_prod_sentiments) * 100
                                                    summary_parts.append(f"{label} {percentage:.0f}% ({count} b√¨nh lu·∫≠n)")
                                            sentiment_summary_for_prompt = ", ".join(summary_parts)
                                            
                                            prompt_prod_summary = f"""S·∫£n ph·∫©m: '{prod_id}'.
T√≥m t·∫Øt c·∫£m x√∫c kh√°ch h√†ng: {sentiment_summary_for_prompt}.
D·ª±a tr√™n t√≥m t·∫Øt n√†y, h√£y ƒë·ªÅ xu·∫•t 2-3 h√†nh ƒë·ªông c·ª• th·ªÉ v√† ∆∞u ti√™n m√† b·ªô ph·∫≠n ChƒÉm s√≥c Kh√°ch h√†ng ho·∫∑c Ph√°t tri·ªÉn S·∫£n ph·∫©m n√™n th·ª±c hi·ªán ƒë·ªÉ c·∫£i thi·ªán tr·∫£i nghi·ªám kh√°ch h√†ng ho·∫∑c s·∫£n ph·∫©m. Tr√¨nh b√†y d∆∞·ªõi d·∫°ng g·∫°ch ƒë·∫ßu d√≤ng ng·∫Øn g·ªçn.
V√≠ d·ª• G·ª£i √Ω:
- [H√†nh ƒë·ªông c·ª• th·ªÉ 1]
- [H√†nh ƒë·ªông c·ª• th·ªÉ 2]"""
                                            
                                            with st.spinner(f"ƒêang l·∫•y g·ª£i √Ω AI cho s·∫£n ph·∫©m {prod_id}..."):
                                                try:
                                                    model_gen_prod = genai.GenerativeModel('gemini-1.5-flash')
                                                    response_gen_prod = model_gen_prod.generate_content(prompt_prod_summary)
                                                    prod_suggestions_text = response_gen_prod.text.strip()
                                                    if prod_suggestions_text:
                                                        # X·ª≠ l√Ω ƒë·ªÉ hi·ªÉn th·ªã ƒë·∫πp h∆°n
                                                        prod_suggestions_list = [s.strip().lstrip("-* ") for s in prod_suggestions_text.split('\n') if s.strip()]
                                                        for sugg_item in prod_suggestions_list:
                                                            st.markdown(f"- {sugg_item}")
                                                    else:
                                                        st.info("AI kh√¥ng ƒë∆∞a ra g·ª£i √Ω c·ª• th·ªÉ.")
                                                except Exception as gemini_e_prod:
                                                    st.warning(f"L·ªói g·ªçi AI cho SP {prod_id}: {gemini_e_prod}")
                                        else:
                                            st.info("Gemini ch∆∞a ƒë∆∞·ª£c c·∫•u h√¨nh ƒë·ªÉ ƒë∆∞a ra g·ª£i √Ω AI.")
                                    else:
                                        st.info("Kh√¥ng c√≥ d·ªØ li·ªáu c·∫£m x√∫c h·ª£p l·ªá cho s·∫£n ph·∫©m n√†y ƒë·ªÉ t·∫°o g·ª£i √Ω.")
                    
                    # --- N√∫t T·∫£i xu·ªëng ---
                    st.markdown("---")
                    st.subheader("üíæ T·∫£i xu·ªëng K·∫øt qu·∫£ Ph√¢n t√≠ch H√†ng lo·∫°t")
                    
                    # S·ª≠ d·ª•ng @st.cache_data cho h√†m convert_df
                    @st.cache_data
                    def convert_df_to_csv_sig(df_to_convert):
                        # Ch·ªçn c√°c c·ªôt c·∫ßn xu·∫•t v√† ƒë√∫ng th·ª© t·ª± mong mu·ªën
                        cols_to_export = [
                            "original_comment", "product_id", "sentiment", "confidence", 
                            "source", "kb_has_ai_details", "status", "would_call_ai", "processing_timestamp"
                        ]
                        # L·∫•y c√°c c·ªôt th·ª±c s·ª± t·ªìn t·∫°i trong df_to_convert ƒë·ªÉ tr√°nh KeyError
                        existing_cols = [col for col in cols_to_export if col in df_to_convert.columns]
                        try:
                            return df_to_convert[existing_cols].to_csv(index=False, encoding='utf-8-sig').encode('utf-8-sig')
                        except Exception as e:
                            st.error(f"L·ªói khi chuy·ªÉn ƒë·ªïi DataFrame sang CSV: {e}")
                            return None

                    csv_data_to_download = convert_df_to_csv_sig(results_df_batch)
                    
                    if csv_data_to_download:
                        st.download_button(
                            label="üì• T·∫£i K·∫øt qu·∫£ (CSV)",
                            data=csv_data_to_download, 
                            file_name=f'ket_qua_phan_tich_cam_xuc_{uploaded_file_batch.name}_{datetime.now().strftime("%Y%m%d_%H%M%S")}.csv',
                            mime='text/csv'
                        )
                    else:
                        st.error("Kh√¥ng th·ªÉ t·∫°o file CSV ƒë·ªÉ t·∫£i xu·ªëng.")
                else:
                    st.warning("Kh√¥ng c√≥ d√≤ng n√†o ƒë∆∞·ª£c x·ª≠ l√Ω, kh√¥ng c√≥ k·∫øt qu·∫£ ƒë·ªÉ hi·ªÉn th·ªã ho·∫∑c t·∫£i xu·ªëng.")
        except Exception as e:
            st.error(f"‚ö†Ô∏è ƒê√£ x·∫£y ra l·ªói kh√¥ng mong mu·ªën trong qu√° tr√¨nh x·ª≠ l√Ω file: {e}")
            traceback.print_exc()

# --- Tab 3: Th√¥ng tin Model (Gi·ªØ nguy√™n) ---
with tab3:
    st.header("Th√¥ng tin ƒê√°nh gi√° Model (XLM-RoBERTa)")
    st.markdown("Ph·∫ßn n√†y hi·ªÉn th·ªã c√°c s·ªë li·ªáu ƒë√°nh gi√° v√† bi·ªÉu ƒë·ªì li√™n quan ƒë·∫øn hi·ªáu su·∫•t c·ªßa m√¥ h√¨nh XLM-RoBERTa ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ ph√¢n t√≠ch c·∫£m x√∫c.")
    
    # C·∫≠p nh·∫≠t: T·∫£i d·ªØ li·ªáu t·ª´ evaluation_summary.json v√† hi·ªÉn th·ªã
    eval_summary_path = os.path.join(config.VISUALIZATION_DIR, 'evaluation_summary.json')
    classification_report_path = os.path.join(config.VISUALIZATION_DIR, 'classification_report.txt')
    confusion_matrix_img_path = config.CONFUSION_MATRIX_FILE
    training_curves_img_path = config.TRAINING_CURVES_FILE

    if os.path.exists(eval_summary_path):
        try:
            with open(eval_summary_path, 'r', encoding='utf-8') as f:
                eval_summary = json.load(f)
            
            st.subheader("üìà S·ªë li·ªáu Hi·ªáu su·∫•t T·ªïng th·ªÉ")
            col_m1, col_m2, col_m3, col_m4 = st.columns(4)
            with col_m1:
                st.metric("Test Accuracy", f"{eval_summary.get('test_accuracy', 0)*100:.2f}%" if eval_summary.get('test_accuracy') is not None else "N/A")
            with col_m2:
                st.metric("Weighted F1-Score", f"{eval_summary.get('weighted_f1', 0):.4f}" if eval_summary.get('weighted_f1') is not None else "N/A")
            with col_m3:
                st.metric("Macro F1-Score", f"{eval_summary.get('macro_f1', 0):.4f}" if eval_summary.get('macro_f1') is not None else "N/A")
            with col_m4:
                st.metric("Test Loss", f"{eval_summary.get('test_loss', 0):.4f}" if eval_summary.get('test_loss') is not None else "N/A")

            if 'classification_report_dict' in eval_summary:
                st.subheader("üìä B√°o c√°o Ph√¢n lo·∫°i Chi ti·∫øt (Test Set)")
                # Hi·ªÉn th·ªã ƒë·∫πp h∆°n classification report d·∫°ng dict
                report_df_data = []
                for label, metrics in eval_summary['classification_report_dict'].items():
                    if isinstance(metrics, dict):
                        report_df_data.append({
                            'C·∫£m x√∫c': label.capitalize(),
                            'Precision': f"{metrics.get('precision',0):.4f}",
                            'Recall': f"{metrics.get('recall',0):.4f}",
                            'F1-Score': f"{metrics.get('f1-score',0):.4f}",
                            'Support': metrics.get('support',0)
                        })
                if report_df_data:
                    report_display_df = pd.DataFrame(report_df_data)
                    st.dataframe(report_display_df.set_index('C·∫£m x√∫c'))
            elif os.path.exists(classification_report_path):
                 st.subheader("üìä B√°o c√°o Ph√¢n lo·∫°i (T·ª´ File)")
                 with open(classification_report_path, 'r', encoding='utf-8') as f:
                     st.text(f.read())
            
        except Exception as e:
            st.error(f"L·ªói khi t·∫£i ho·∫∑c hi·ªÉn th·ªã t√≥m t·∫Øt ƒë√°nh gi√°: {e}")
    else:
        st.warning(f"Kh√¥ng t√¨m th·∫•y file t√≥m t·∫Øt ƒë√°nh gi√° t·∫°i: {eval_summary_path}. Vui l√≤ng ch·∫°y 'evaluate.py' tr∆∞·ªõc.")

    st.subheader("üñºÔ∏è Bi·ªÉu ƒë·ªì Hi·ªáu su·∫•t")
    col_img1, col_img2 = st.columns(2)
    with col_img1:
        if os.path.exists(confusion_matrix_img_path):
            st.image(confusion_matrix_img_path, caption="Ma tr·∫≠n Nh·∫ßm l·∫´n (Test Set)", use_column_width=True)
        else:
            st.caption(f"Kh√¥ng t√¨m th·∫•y h√¨nh ·∫£nh Ma tr·∫≠n nh·∫ßm l·∫´n. Ch·∫°y 'evaluate.py'. ({confusion_matrix_img_path})")
    
    with col_img2:
        if os.path.exists(training_curves_img_path):
            st.image(training_curves_img_path, caption="Bi·ªÉu ƒë·ªì Hu·∫•n luy·ªán (Loss & Accuracy)", use_column_width=True)
        else:
            st.caption(f"Kh√¥ng t√¨m th·∫•y h√¨nh ·∫£nh Bi·ªÉu ƒë·ªì hu·∫•n luy·ªán. Ch·∫°y 'evaluate.py'. ({training_curves_img_path})")
    
    # Hi·ªÉn th·ªã m·ªôt v√†i v√≠ d·ª• l·ªói n·∫øu c√≥
    if os.path.exists(eval_summary_path) and eval_summary.get('error_samples_examples'):
        st.subheader("üßê V√≠ d·ª• c√°c M·∫´u D·ª± ƒëo√°n Sai (Test Set)")
        error_examples_df = pd.DataFrame(eval_summary['error_samples_examples'])
        st.dataframe(error_examples_df[['cleaned_text', 'true_label_name', 'predicted_label_name']].rename(columns={
            'cleaned_text': 'B√¨nh lu·∫≠n ƒë√£ l√†m s·∫°ch',
            'true_label_name': 'Nh√£n th·ª±c t·∫ø',
            'predicted_label_name': 'Nh√£n d·ª± ƒëo√°n'
        }))


# --- Footer ---
st.markdown("---")
st.caption("D·ª± √°n Th·ª±c t·∫≠p - X·ª≠ l√Ω Ph·∫£n h·ªìi Kh√°ch h√†ng - [Nguy·ªÖn Tr·∫ßn Ho√†ng Th·ªãnh]")